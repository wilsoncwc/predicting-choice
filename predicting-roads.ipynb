{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7958681-6281-4f3f-92b8-ee6f2a1c8c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSx metrics of 348 local authorities retrieved\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "import momepy\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "\n",
    "from torch_geometric.data import Data, Batch, InMemoryDataset, download_url, extract_zip\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import RandomLinkSplit, OneHotDegree\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n",
    "\n",
    "from train import run, run_single\n",
    "from utils.load_geodata import load_gdf, load_graph, process_graph\n",
    "from utils.constants import project_root, dataset_root\n",
    "from utils.constants import rank_fields, log_fields, all_feature_fields, feats, included_places\n",
    "from utils.valid_edge import is_valid\n",
    "\n",
    "print(f'SSx metrics of {len(included_places)} local authorities retrieved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dd4a1-4dde-4c33-bee7-f307078c4d99",
   "metadata": {},
   "source": [
    "## Train on multiple LAs and test on a hold-out set of LAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0339c468-2708-446c-ba8f-0a9e7116a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gin\n",
      "Running iteration 1 of expt GAE_10d_gin-dist_20epochs_0.01lr_2_feats\n",
      "Initialized ModGAE(\n",
      "  (encoder): GCNEncoder(\n",
      "    (conv): GIN(2, 10, num_layers=2)\n",
      "  )\n",
      "  (decoder): DistMultDecoder()\n",
      ") with arguments {'out_channels': 10, 'model_type': 'gin', 'num_layers': 2, 'in_channels': 2}\n",
      "Total number of parameters: 500\n",
      "Epoch 010 (1.57s/epoch): Train AUC: 0.7364, Train AP: 0.2742,Test AUC: 0.7315, Test AP: 0.2702\n",
      "Epoch 020 (1.57s/epoch): Train AUC: 0.7572, Train AP: 0.3006,Test AUC: 0.7514, Test AP: 0.2939\n",
      "Iteration 1 results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_loss</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_ap</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.757156</td>\n",
       "      <td>0.300554</td>\n",
       "      <td>0.751444</td>\n",
       "      <td>0.293871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_loss  train_auc  train_ap  test_auc   test_ap\n",
       "0    0.015766   0.757156  0.300554  0.751444  0.293871"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModGAE' object has no attribute 'test_curve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m proc_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_feats\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintegration2kmrank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintegration10kmrank\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_deg_feats\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m models, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mproc_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mschedule_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_tb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/predicting-choice/train.py:226\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(dataset, data_process_args, model_args, seed, num_iter, save_best_model, save_best_model_metric, output_tb, tag_tb, epochs, print_every, lr, schedule_lr, beta)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m results:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    225\u001b[0m display(result_df)\n\u001b[0;32m--> 226\u001b[0m roc_curve, pr_curve \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_curve\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m roc_curve\n\u001b[1;32m    228\u001b[0m result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_curve\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pr_curve\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/predicting-choice/train.py:122\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, curve)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    121\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m--> 122\u001b[0m     test_fn \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_curve\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m curve \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtest\n\u001b[1;32m    123\u001b[0m     auc, ap \u001b[38;5;241m=\u001b[39m test_fn(z, data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, data\u001b[38;5;241m.\u001b[39mneg_edge_label_index)\n\u001b[1;32m    124\u001b[0m     auc_roc\u001b[38;5;241m.\u001b[39mappend(auc)\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModGAE' object has no attribute 'test_curve'"
     ]
    }
   ],
   "source": [
    "dataset_conn = torch.load(f'{dataset_root}/ssx_dataset_connected.pt')\n",
    "\n",
    "for model_type in ['gin', 'gain']:\n",
    "    model_args = {\n",
    "        'out_channels': 10,\n",
    "        'model_type': model_type,\n",
    "        'num_layers': 2,\n",
    "        'distmult': True\n",
    "    }\n",
    "    proc_args = {\n",
    "        'include_feats': ['integration2kmrank', 'integration10kmrank'],\n",
    "        'add_deg_feats': False\n",
    "    }\n",
    "    print(f'Testing {model_type}')\n",
    "    models, results = run(dataset_conn,\n",
    "                        proc_args,\n",
    "                        model_args,\n",
    "                        num_iter=3,\n",
    "                        lr=0.01,\n",
    "                        epochs=20,\n",
    "                        schedule_lr=False,\n",
    "                        output_tb=False,\n",
    "                        save_best_model=False)\n",
    "# torch.save(dict_res, './run_results_16_04_22.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670af72-d298-44ea-8fa7-ee6ae8fd861e",
   "metadata": {},
   "source": [
    "## Train on a single LA's graph and test against all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c7d41b-b060-43bd-8445-e15cddf37977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on SSx data from Poole...\n",
      "Running iteration 1 of expt GAE_10d_gat_1000epochs_0.01lr_4_feats\n",
      "{'in_channels': 4, 'out_channels': 10, 'hidden_channels': 10, 'num_layers': 2, 'jk': 'cat'}\n",
      "Epoch 010 (1.78s/epoch): Train AUC: 0.5748, Train AP: 0.2596,Test AUC: 0.5595, Test AP: 0.2322\n",
      "Epoch 020 (1.77s/epoch): Train AUC: 0.5990, Train AP: 0.2739,Test AUC: 0.5734, Test AP: 0.2509\n",
      "Epoch 030 (1.78s/epoch): Train AUC: 0.6793, Train AP: 0.2943,Test AUC: 0.6467, Test AP: 0.2845\n",
      "Epoch 040 (1.77s/epoch): Train AUC: 0.6965, Train AP: 0.3205,Test AUC: 0.6590, Test AP: 0.2899\n",
      "Epoch 050 (1.78s/epoch): Train AUC: 0.7516, Train AP: 0.3583,Test AUC: 0.7070, Test AP: 0.3165\n",
      "Epoch 060 (1.83s/epoch): Train AUC: 0.7925, Train AP: 0.4081,Test AUC: 0.7388, Test AP: 0.3400\n",
      "Epoch 070 (1.77s/epoch): Train AUC: 0.8038, Train AP: 0.4105,Test AUC: 0.7587, Test AP: 0.3727\n",
      "Epoch 080 (1.78s/epoch): Train AUC: 0.8116, Train AP: 0.4311,Test AUC: 0.7637, Test AP: 0.3764\n",
      "Epoch 090 (1.77s/epoch): Train AUC: 0.8199, Train AP: 0.4317,Test AUC: 0.7806, Test AP: 0.3877\n",
      "Epoch 100 (1.77s/epoch): Train AUC: 0.8217, Train AP: 0.4417,Test AUC: 0.7807, Test AP: 0.3842\n",
      "Epoch 110 (1.77s/epoch): Train AUC: 0.8241, Train AP: 0.4502,Test AUC: 0.7842, Test AP: 0.3862\n",
      "Epoch 120 (1.78s/epoch): Train AUC: 0.8214, Train AP: 0.4313,Test AUC: 0.7829, Test AP: 0.3784\n",
      "Epoch 130 (1.77s/epoch): Train AUC: 0.8243, Train AP: 0.4309,Test AUC: 0.7874, Test AP: 0.3800\n",
      "Epoch 140 (1.77s/epoch): Train AUC: 0.8303, Train AP: 0.4437,Test AUC: 0.7957, Test AP: 0.3956\n",
      "Epoch 150 (1.76s/epoch): Train AUC: 0.8347, Train AP: 0.4489,Test AUC: 0.8035, Test AP: 0.4180\n",
      "Epoch 160 (1.78s/epoch): Train AUC: 0.8421, Train AP: 0.4845,Test AUC: 0.8075, Test AP: 0.4217\n",
      "Epoch 170 (1.78s/epoch): Train AUC: 0.8411, Train AP: 0.4750,Test AUC: 0.8120, Test AP: 0.4286\n",
      "Epoch 180 (1.77s/epoch): Train AUC: 0.8419, Train AP: 0.4791,Test AUC: 0.8136, Test AP: 0.4344\n",
      "Epoch 190 (1.77s/epoch): Train AUC: 0.8423, Train AP: 0.4781,Test AUC: 0.8131, Test AP: 0.4302\n",
      "Epoch 200 (1.77s/epoch): Train AUC: 0.8434, Train AP: 0.4756,Test AUC: 0.8151, Test AP: 0.4346\n",
      "Epoch 210 (1.78s/epoch): Train AUC: 0.8457, Train AP: 0.4807,Test AUC: 0.8187, Test AP: 0.4415\n",
      "Epoch 220 (1.77s/epoch): Train AUC: 0.8468, Train AP: 0.4816,Test AUC: 0.8200, Test AP: 0.4435\n",
      "Epoch 230 (1.77s/epoch): Train AUC: 0.8495, Train AP: 0.4846,Test AUC: 0.8223, Test AP: 0.4470\n",
      "Epoch 240 (1.77s/epoch): Train AUC: 0.8522, Train AP: 0.4888,Test AUC: 0.8248, Test AP: 0.4536\n",
      "Epoch 250 (1.77s/epoch): Train AUC: 0.8534, Train AP: 0.4892,Test AUC: 0.8243, Test AP: 0.4496\n",
      "Epoch 260 (1.77s/epoch): Train AUC: 0.8552, Train AP: 0.4931,Test AUC: 0.8254, Test AP: 0.4518\n",
      "Epoch 270 (1.77s/epoch): Train AUC: 0.8582, Train AP: 0.5062,Test AUC: 0.8275, Test AP: 0.4566\n",
      "Epoch 280 (1.77s/epoch): Train AUC: 0.8528, Train AP: 0.4949,Test AUC: 0.8119, Test AP: 0.4261\n",
      "Epoch 290 (1.76s/epoch): Train AUC: 0.8521, Train AP: 0.4919,Test AUC: 0.8183, Test AP: 0.4488\n",
      "Epoch 300 (1.78s/epoch): Train AUC: 0.8597, Train AP: 0.5195,Test AUC: 0.8276, Test AP: 0.4587\n",
      "Epoch 310 (1.77s/epoch): Train AUC: 0.8595, Train AP: 0.5308,Test AUC: 0.8278, Test AP: 0.4606\n",
      "Epoch 320 (1.77s/epoch): Train AUC: 0.8591, Train AP: 0.5275,Test AUC: 0.8299, Test AP: 0.4648\n",
      "Epoch 330 (1.77s/epoch): Train AUC: 0.8617, Train AP: 0.5350,Test AUC: 0.8318, Test AP: 0.4637\n",
      "Epoch 340 (1.78s/epoch): Train AUC: 0.8626, Train AP: 0.5311,Test AUC: 0.8330, Test AP: 0.4664\n",
      "Epoch 350 (1.77s/epoch): Train AUC: 0.8628, Train AP: 0.5325,Test AUC: 0.8328, Test AP: 0.4649\n",
      "Epoch 360 (1.77s/epoch): Train AUC: 0.8640, Train AP: 0.5340,Test AUC: 0.8334, Test AP: 0.4656\n",
      "Epoch 370 (1.77s/epoch): Train AUC: 0.8650, Train AP: 0.5351,Test AUC: 0.8341, Test AP: 0.4661\n",
      "Epoch 380 (1.76s/epoch): Train AUC: 0.8659, Train AP: 0.5360,Test AUC: 0.8345, Test AP: 0.4662\n",
      "Epoch 390 (1.77s/epoch): Train AUC: 0.8667, Train AP: 0.5375,Test AUC: 0.8350, Test AP: 0.4667\n",
      "Epoch 400 (1.77s/epoch): Train AUC: 0.8674, Train AP: 0.5388,Test AUC: 0.8354, Test AP: 0.4668\n",
      "Epoch 410 (1.77s/epoch): Train AUC: 0.8682, Train AP: 0.5397,Test AUC: 0.8359, Test AP: 0.4669\n",
      "Epoch 420 (1.77s/epoch): Train AUC: 0.8692, Train AP: 0.5428,Test AUC: 0.8364, Test AP: 0.4674\n",
      "Epoch 430 (1.76s/epoch): Train AUC: 0.8698, Train AP: 0.5438,Test AUC: 0.8368, Test AP: 0.4677\n",
      "Epoch 440 (1.75s/epoch): Train AUC: 0.8702, Train AP: 0.5443,Test AUC: 0.8372, Test AP: 0.4679\n",
      "Epoch 450 (1.76s/epoch): Train AUC: 0.8706, Train AP: 0.5452,Test AUC: 0.8373, Test AP: 0.4678\n",
      "Epoch 460 (1.76s/epoch): Train AUC: 0.8710, Train AP: 0.5468,Test AUC: 0.8374, Test AP: 0.4674\n",
      "Epoch 470 (1.75s/epoch): Train AUC: 0.8715, Train AP: 0.5495,Test AUC: 0.8377, Test AP: 0.4674\n",
      "Epoch 480 (1.76s/epoch): Train AUC: 0.8714, Train AP: 0.5497,Test AUC: 0.8373, Test AP: 0.4659\n",
      "Epoch 490 (1.75s/epoch): Train AUC: 0.8720, Train AP: 0.5522,Test AUC: 0.8374, Test AP: 0.4650\n",
      "Epoch 500 (1.75s/epoch): Train AUC: 0.8728, Train AP: 0.5543,Test AUC: 0.8375, Test AP: 0.4641\n",
      "Epoch 510 (1.75s/epoch): Train AUC: 0.8731, Train AP: 0.5556,Test AUC: 0.8373, Test AP: 0.4630\n",
      "Epoch 520 (1.75s/epoch): Train AUC: 0.8734, Train AP: 0.5559,Test AUC: 0.8372, Test AP: 0.4620\n",
      "Epoch 530 (1.76s/epoch): Train AUC: 0.8744, Train AP: 0.5597,Test AUC: 0.8376, Test AP: 0.4621\n",
      "Epoch 540 (1.75s/epoch): Train AUC: 0.8760, Train AP: 0.5643,Test AUC: 0.8391, Test AP: 0.4655\n",
      "Epoch 550 (1.76s/epoch): Train AUC: 0.8755, Train AP: 0.5683,Test AUC: 0.8374, Test AP: 0.4603\n",
      "Epoch 560 (1.76s/epoch): Train AUC: 0.8762, Train AP: 0.5680,Test AUC: 0.8344, Test AP: 0.4546\n",
      "Epoch 570 (1.76s/epoch): Train AUC: 0.8630, Train AP: 0.5487,Test AUC: 0.8267, Test AP: 0.4452\n",
      "Epoch 580 (1.76s/epoch): Train AUC: 0.8766, Train AP: 0.5528,Test AUC: 0.8369, Test AP: 0.4597\n",
      "Epoch 590 (1.77s/epoch): Train AUC: 0.8752, Train AP: 0.5691,Test AUC: 0.8340, Test AP: 0.4586\n",
      "Epoch 600 (1.77s/epoch): Train AUC: 0.8806, Train AP: 0.5705,Test AUC: 0.8393, Test AP: 0.4655\n",
      "Epoch 610 (1.76s/epoch): Train AUC: 0.8789, Train AP: 0.5787,Test AUC: 0.8375, Test AP: 0.4622\n",
      "Epoch 620 (1.76s/epoch): Train AUC: 0.8778, Train AP: 0.5763,Test AUC: 0.8389, Test AP: 0.4669\n",
      "Epoch 630 (1.76s/epoch): Train AUC: 0.8788, Train AP: 0.5798,Test AUC: 0.8394, Test AP: 0.4665\n",
      "Epoch 640 (1.76s/epoch): Train AUC: 0.8779, Train AP: 0.5747,Test AUC: 0.8394, Test AP: 0.4666\n",
      "Epoch 650 (1.76s/epoch): Train AUC: 0.8782, Train AP: 0.5753,Test AUC: 0.8399, Test AP: 0.4672\n",
      "Epoch 660 (1.76s/epoch): Train AUC: 0.8786, Train AP: 0.5771,Test AUC: 0.8401, Test AP: 0.4673\n",
      "Epoch 670 (1.76s/epoch): Train AUC: 0.8791, Train AP: 0.5781,Test AUC: 0.8403, Test AP: 0.4671\n",
      "Epoch 680 (1.76s/epoch): Train AUC: 0.8795, Train AP: 0.5784,Test AUC: 0.8406, Test AP: 0.4674\n",
      "Epoch 690 (1.77s/epoch): Train AUC: 0.8798, Train AP: 0.5797,Test AUC: 0.8409, Test AP: 0.4679\n",
      "Epoch 700 (1.76s/epoch): Train AUC: 0.8802, Train AP: 0.5801,Test AUC: 0.8411, Test AP: 0.4682\n",
      "Epoch 710 (1.76s/epoch): Train AUC: 0.8805, Train AP: 0.5809,Test AUC: 0.8413, Test AP: 0.4683\n",
      "Epoch 720 (1.75s/epoch): Train AUC: 0.8808, Train AP: 0.5816,Test AUC: 0.8414, Test AP: 0.4684\n",
      "Epoch 730 (1.75s/epoch): Train AUC: 0.8811, Train AP: 0.5821,Test AUC: 0.8418, Test AP: 0.4690\n",
      "Epoch 740 (1.75s/epoch): Train AUC: 0.8815, Train AP: 0.5831,Test AUC: 0.8419, Test AP: 0.4689\n",
      "Epoch 750 (1.75s/epoch): Train AUC: 0.8818, Train AP: 0.5837,Test AUC: 0.8420, Test AP: 0.4687\n",
      "Epoch 760 (1.75s/epoch): Train AUC: 0.8822, Train AP: 0.5848,Test AUC: 0.8422, Test AP: 0.4687\n",
      "Epoch 770 (1.75s/epoch): Train AUC: 0.8824, Train AP: 0.5862,Test AUC: 0.8422, Test AP: 0.4685\n",
      "Epoch 780 (1.75s/epoch): Train AUC: 0.8826, Train AP: 0.5864,Test AUC: 0.8424, Test AP: 0.4685\n",
      "Epoch 790 (1.76s/epoch): Train AUC: 0.8828, Train AP: 0.5869,Test AUC: 0.8425, Test AP: 0.4683\n",
      "Epoch 800 (1.75s/epoch): Train AUC: 0.8823, Train AP: 0.5864,Test AUC: 0.8424, Test AP: 0.4683\n",
      "Epoch 810 (1.75s/epoch): Train AUC: 0.8823, Train AP: 0.5877,Test AUC: 0.8423, Test AP: 0.4679\n",
      "Epoch 820 (1.75s/epoch): Train AUC: 0.8826, Train AP: 0.5884,Test AUC: 0.8424, Test AP: 0.4677\n",
      "Epoch 830 (1.75s/epoch): Train AUC: 0.8829, Train AP: 0.5892,Test AUC: 0.8425, Test AP: 0.4675\n",
      "Epoch 840 (1.75s/epoch): Train AUC: 0.8831, Train AP: 0.5891,Test AUC: 0.8426, Test AP: 0.4675\n",
      "Epoch 850 (1.75s/epoch): Train AUC: 0.8834, Train AP: 0.5891,Test AUC: 0.8427, Test AP: 0.4674\n",
      "Epoch 860 (1.75s/epoch): Train AUC: 0.8836, Train AP: 0.5888,Test AUC: 0.8424, Test AP: 0.4662\n",
      "Epoch 870 (1.75s/epoch): Train AUC: 0.8850, Train AP: 0.5902,Test AUC: 0.8445, Test AP: 0.4719\n",
      "Epoch 880 (1.74s/epoch): Train AUC: 0.8843, Train AP: 0.5924,Test AUC: 0.8427, Test AP: 0.4670\n",
      "Epoch 890 (1.74s/epoch): Train AUC: 0.8851, Train AP: 0.5953,Test AUC: 0.8439, Test AP: 0.4703\n",
      "Epoch 900 (1.74s/epoch): Train AUC: 0.8855, Train AP: 0.5966,Test AUC: 0.8444, Test AP: 0.4714\n",
      "Epoch 910 (1.73s/epoch): Train AUC: 0.8859, Train AP: 0.5969,Test AUC: 0.8451, Test AP: 0.4733\n",
      "Epoch 920 (1.73s/epoch): Train AUC: 0.8854, Train AP: 0.5976,Test AUC: 0.8441, Test AP: 0.4704\n",
      "Epoch 930 (1.73s/epoch): Train AUC: 0.8875, Train AP: 0.5972,Test AUC: 0.8465, Test AP: 0.4777\n",
      "Epoch 940 (1.73s/epoch): Train AUC: 0.8849, Train AP: 0.5795,Test AUC: 0.8407, Test AP: 0.4553\n",
      "Epoch 950 (1.74s/epoch): Train AUC: 0.8830, Train AP: 0.5800,Test AUC: 0.8371, Test AP: 0.4608\n",
      "Epoch 960 (1.75s/epoch): Train AUC: 0.8920, Train AP: 0.5966,Test AUC: 0.8383, Test AP: 0.4548\n",
      "Epoch 970 (1.75s/epoch): Train AUC: 0.8896, Train AP: 0.6016,Test AUC: 0.8423, Test AP: 0.4668\n",
      "Epoch 980 (1.77s/epoch): Train AUC: 0.8888, Train AP: 0.6044,Test AUC: 0.8502, Test AP: 0.4910\n",
      "Epoch 990 (1.76s/epoch): Train AUC: 0.8870, Train AP: 0.5974,Test AUC: 0.8433, Test AP: 0.4714\n",
      "Epoch 1000 (1.75s/epoch): Train AUC: 0.8845, Train AP: 0.5879,Test AUC: 0.8435, Test AP: 0.4757\n",
      "Running iteration 2 of expt GAE_10d_gat_1000epochs_0.01lr_4_feats\n",
      "{'in_channels': 4, 'out_channels': 10, 'hidden_channels': 10, 'num_layers': 2, 'jk': 'cat'}\n",
      "Epoch 010 (1.77s/epoch): Train AUC: 0.5602, Train AP: 0.2421,Test AUC: 0.5536, Test AP: 0.2313\n",
      "Epoch 020 (1.78s/epoch): Train AUC: 0.6526, Train AP: 0.2699,Test AUC: 0.6326, Test AP: 0.2610\n",
      "Epoch 030 (1.78s/epoch): Train AUC: 0.6676, Train AP: 0.2841,Test AUC: 0.6455, Test AP: 0.2687\n",
      "Epoch 040 (1.79s/epoch): Train AUC: 0.6761, Train AP: 0.2927,Test AUC: 0.6569, Test AP: 0.2764\n",
      "Epoch 050 (1.78s/epoch): Train AUC: 0.6848, Train AP: 0.3017,Test AUC: 0.6639, Test AP: 0.2812\n",
      "Epoch 060 (1.78s/epoch): Train AUC: 0.7024, Train AP: 0.3090,Test AUC: 0.6788, Test AP: 0.2897\n",
      "Epoch 070 (1.85s/epoch): Train AUC: 0.7125, Train AP: 0.3322,Test AUC: 0.7011, Test AP: 0.3099\n",
      "Epoch 080 (1.87s/epoch): Train AUC: 0.7147, Train AP: 0.3168,Test AUC: 0.7102, Test AP: 0.3031\n",
      "Epoch 090 (1.83s/epoch): Train AUC: 0.7093, Train AP: 0.3347,Test AUC: 0.7099, Test AP: 0.3228\n",
      "Epoch 100 (1.78s/epoch): Train AUC: 0.7172, Train AP: 0.3415,Test AUC: 0.7160, Test AP: 0.3242\n",
      "Epoch 110 (1.78s/epoch): Train AUC: 0.7259, Train AP: 0.3386,Test AUC: 0.7205, Test AP: 0.3223\n",
      "Epoch 120 (1.78s/epoch): Train AUC: 0.7294, Train AP: 0.3230,Test AUC: 0.7198, Test AP: 0.3089\n",
      "Epoch 130 (1.78s/epoch): Train AUC: 0.7298, Train AP: 0.3122,Test AUC: 0.7182, Test AP: 0.3025\n",
      "Epoch 140 (1.78s/epoch): Train AUC: 0.7427, Train AP: 0.3416,Test AUC: 0.7295, Test AP: 0.3253\n",
      "Epoch 150 (1.78s/epoch): Train AUC: 0.7494, Train AP: 0.3517,Test AUC: 0.7327, Test AP: 0.3291\n",
      "Epoch 160 (1.78s/epoch): Train AUC: 0.7525, Train AP: 0.3447,Test AUC: 0.7314, Test AP: 0.3207\n",
      "Epoch 170 (1.78s/epoch): Train AUC: 0.7547, Train AP: 0.3459,Test AUC: 0.7325, Test AP: 0.3209\n",
      "Epoch 180 (1.78s/epoch): Train AUC: 0.7576, Train AP: 0.3477,Test AUC: 0.7342, Test AP: 0.3241\n",
      "Epoch 190 (1.76s/epoch): Train AUC: 0.7633, Train AP: 0.3649,Test AUC: 0.7393, Test AP: 0.3347\n",
      "Epoch 200 (1.77s/epoch): Train AUC: 0.7594, Train AP: 0.3585,Test AUC: 0.7404, Test AP: 0.3358\n",
      "Epoch 210 (1.77s/epoch): Train AUC: 0.7631, Train AP: 0.3598,Test AUC: 0.7385, Test AP: 0.3287\n",
      "Epoch 220 (1.83s/epoch): Train AUC: 0.7625, Train AP: 0.3515,Test AUC: 0.7376, Test AP: 0.3253\n",
      "Epoch 230 (1.82s/epoch): Train AUC: 0.7642, Train AP: 0.3575,Test AUC: 0.7391, Test AP: 0.3290\n",
      "Epoch 240 (1.91s/epoch): Train AUC: 0.7643, Train AP: 0.3545,Test AUC: 0.7378, Test AP: 0.3242\n",
      "Epoch 250 (1.91s/epoch): Train AUC: 0.7646, Train AP: 0.3544,Test AUC: 0.7378, Test AP: 0.3231\n",
      "Epoch 260 (1.88s/epoch): Train AUC: 0.7653, Train AP: 0.3549,Test AUC: 0.7381, Test AP: 0.3229\n",
      "Epoch 270 (1.87s/epoch): Train AUC: 0.7657, Train AP: 0.3544,Test AUC: 0.7382, Test AP: 0.3222\n",
      "Epoch 280 (1.81s/epoch): Train AUC: 0.7665, Train AP: 0.3550,Test AUC: 0.7387, Test AP: 0.3225\n",
      "Epoch 290 (1.80s/epoch): Train AUC: 0.7670, Train AP: 0.3544,Test AUC: 0.7387, Test AP: 0.3214\n",
      "Epoch 300 (1.81s/epoch): Train AUC: 0.7675, Train AP: 0.3543,Test AUC: 0.7387, Test AP: 0.3206\n",
      "Epoch 310 (1.78s/epoch): Train AUC: 0.7680, Train AP: 0.3543,Test AUC: 0.7384, Test AP: 0.3193\n",
      "Epoch 320 (1.78s/epoch): Train AUC: 0.7691, Train AP: 0.3563,Test AUC: 0.7388, Test AP: 0.3190\n",
      "Epoch 330 (1.77s/epoch): Train AUC: 0.7716, Train AP: 0.3757,Test AUC: 0.7431, Test AP: 0.3334\n",
      "Epoch 340 (1.78s/epoch): Train AUC: 0.7726, Train AP: 0.3681,Test AUC: 0.7401, Test AP: 0.3224\n",
      "Epoch 350 (1.78s/epoch): Train AUC: 0.7706, Train AP: 0.3582,Test AUC: 0.7370, Test AP: 0.3144\n",
      "Epoch 360 (1.78s/epoch): Train AUC: 0.7717, Train AP: 0.3661,Test AUC: 0.7399, Test AP: 0.3218\n",
      "Epoch 370 (1.78s/epoch): Train AUC: 0.7711, Train AP: 0.3584,Test AUC: 0.7380, Test AP: 0.3155\n",
      "Epoch 380 (1.76s/epoch): Train AUC: 0.7710, Train AP: 0.3575,Test AUC: 0.7375, Test AP: 0.3140\n",
      "Epoch 390 (1.77s/epoch): Train AUC: 0.7721, Train AP: 0.3639,Test AUC: 0.7389, Test AP: 0.3178\n",
      "Epoch 400 (1.77s/epoch): Train AUC: 0.7715, Train AP: 0.3612,Test AUC: 0.7383, Test AP: 0.3151\n",
      "Epoch 410 (1.77s/epoch): Train AUC: 0.7698, Train AP: 0.3505,Test AUC: 0.7359, Test AP: 0.3081\n",
      "Epoch 420 (1.77s/epoch): Train AUC: 0.7712, Train AP: 0.3562,Test AUC: 0.7375, Test AP: 0.3118\n",
      "Epoch 430 (1.77s/epoch): Train AUC: 0.7721, Train AP: 0.3644,Test AUC: 0.7391, Test AP: 0.3179\n",
      "Epoch 440 (1.76s/epoch): Train AUC: 0.7725, Train AP: 0.3598,Test AUC: 0.7382, Test AP: 0.3136\n",
      "Epoch 450 (1.76s/epoch): Train AUC: 0.7725, Train AP: 0.3591,Test AUC: 0.7381, Test AP: 0.3132\n",
      "Epoch 460 (1.76s/epoch): Train AUC: 0.7726, Train AP: 0.3621,Test AUC: 0.7390, Test AP: 0.3162\n",
      "Epoch 470 (1.77s/epoch): Train AUC: 0.7721, Train AP: 0.3558,Test AUC: 0.7364, Test AP: 0.3088\n",
      "Epoch 480 (1.76s/epoch): Train AUC: 0.7721, Train AP: 0.3541,Test AUC: 0.7366, Test AP: 0.3086\n",
      "Epoch 490 (1.78s/epoch): Train AUC: 0.7730, Train AP: 0.3590,Test AUC: 0.7372, Test AP: 0.3105\n",
      "Epoch 500 (1.78s/epoch): Train AUC: 0.7729, Train AP: 0.3590,Test AUC: 0.7376, Test AP: 0.3112\n",
      "Epoch 510 (1.78s/epoch): Train AUC: 0.7746, Train AP: 0.3631,Test AUC: 0.7385, Test AP: 0.3135\n",
      "Epoch 520 (1.78s/epoch): Train AUC: 0.7743, Train AP: 0.3659,Test AUC: 0.7396, Test AP: 0.3171\n",
      "Epoch 530 (1.77s/epoch): Train AUC: 0.7731, Train AP: 0.3561,Test AUC: 0.7370, Test AP: 0.3085\n",
      "Epoch 540 (1.77s/epoch): Train AUC: 0.7731, Train AP: 0.3561,Test AUC: 0.7367, Test AP: 0.3080\n",
      "Epoch 550 (1.77s/epoch): Train AUC: 0.7750, Train AP: 0.3647,Test AUC: 0.7384, Test AP: 0.3134\n",
      "Epoch 560 (1.79s/epoch): Train AUC: 0.7745, Train AP: 0.3605,Test AUC: 0.7376, Test AP: 0.3104\n",
      "Epoch 570 (1.79s/epoch): Train AUC: 0.7747, Train AP: 0.3599,Test AUC: 0.7371, Test AP: 0.3091\n",
      "Epoch 580 (1.81s/epoch): Train AUC: 0.7740, Train AP: 0.3578,Test AUC: 0.7369, Test AP: 0.3084\n",
      "Epoch 590 (1.77s/epoch): Train AUC: 0.7742, Train AP: 0.3568,Test AUC: 0.7368, Test AP: 0.3082\n",
      "Epoch 600 (1.77s/epoch): Train AUC: 0.7751, Train AP: 0.3622,Test AUC: 0.7378, Test AP: 0.3109\n",
      "Epoch 610 (1.80s/epoch): Train AUC: 0.7759, Train AP: 0.3629,Test AUC: 0.7375, Test AP: 0.3101\n",
      "Epoch 620 (1.77s/epoch): Train AUC: 0.7751, Train AP: 0.3619,Test AUC: 0.7379, Test AP: 0.3115\n",
      "Epoch 630 (1.77s/epoch): Train AUC: 0.7744, Train AP: 0.3544,Test AUC: 0.7354, Test AP: 0.3051\n",
      "Epoch 640 (1.77s/epoch): Train AUC: 0.7751, Train AP: 0.3585,Test AUC: 0.7362, Test AP: 0.3070\n",
      "Epoch 650 (1.76s/epoch): Train AUC: 0.7758, Train AP: 0.3610,Test AUC: 0.7369, Test AP: 0.3088\n",
      "Epoch 660 (1.78s/epoch): Train AUC: 0.7763, Train AP: 0.3641,Test AUC: 0.7376, Test AP: 0.3108\n",
      "Epoch 670 (1.77s/epoch): Train AUC: 0.7751, Train AP: 0.3569,Test AUC: 0.7358, Test AP: 0.3061\n",
      "Epoch 680 (1.77s/epoch): Train AUC: 0.7749, Train AP: 0.3540,Test AUC: 0.7354, Test AP: 0.3044\n",
      "Epoch 690 (1.78s/epoch): Train AUC: 0.7748, Train AP: 0.3546,Test AUC: 0.7359, Test AP: 0.3056\n",
      "Epoch 700 (1.77s/epoch): Train AUC: 0.7755, Train AP: 0.3566,Test AUC: 0.7359, Test AP: 0.3058\n",
      "Epoch 710 (1.77s/epoch): Train AUC: 0.7765, Train AP: 0.3605,Test AUC: 0.7365, Test AP: 0.3076\n",
      "Epoch 720 (1.77s/epoch): Train AUC: 0.7774, Train AP: 0.3644,Test AUC: 0.7376, Test AP: 0.3104\n",
      "Epoch 730 (1.78s/epoch): Train AUC: 0.7771, Train AP: 0.3619,Test AUC: 0.7367, Test AP: 0.3082\n",
      "Epoch 740 (1.77s/epoch): Train AUC: 0.7769, Train AP: 0.3607,Test AUC: 0.7369, Test AP: 0.3083\n",
      "Epoch 750 (1.77s/epoch): Train AUC: 0.7766, Train AP: 0.3590,Test AUC: 0.7361, Test AP: 0.3064\n",
      "Epoch 760 (1.77s/epoch): Train AUC: 0.7776, Train AP: 0.3629,Test AUC: 0.7367, Test AP: 0.3084\n",
      "Epoch 770 (1.77s/epoch): Train AUC: 0.7778, Train AP: 0.3646,Test AUC: 0.7379, Test AP: 0.3113\n",
      "Epoch 780 (1.77s/epoch): Train AUC: 0.7779, Train AP: 0.3630,Test AUC: 0.7374, Test AP: 0.3097\n",
      "Epoch 790 (1.77s/epoch): Train AUC: 0.7785, Train AP: 0.3670,Test AUC: 0.7380, Test AP: 0.3117\n",
      "Epoch 800 (1.77s/epoch): Train AUC: 0.7792, Train AP: 0.3789,Test AUC: 0.7404, Test AP: 0.3213\n",
      "Epoch 810 (1.77s/epoch): Train AUC: 0.7806, Train AP: 0.3783,Test AUC: 0.7404, Test AP: 0.3196\n",
      "Epoch 820 (1.77s/epoch): Train AUC: 0.7804, Train AP: 0.3834,Test AUC: 0.7416, Test AP: 0.3250\n",
      "Epoch 830 (1.77s/epoch): Train AUC: 0.7834, Train AP: 0.3940,Test AUC: 0.7429, Test AP: 0.3291\n",
      "Epoch 840 (1.77s/epoch): Train AUC: 0.7816, Train AP: 0.3843,Test AUC: 0.7404, Test AP: 0.3207\n",
      "Epoch 850 (1.78s/epoch): Train AUC: 0.7804, Train AP: 0.3748,Test AUC: 0.7390, Test AP: 0.3156\n",
      "Epoch 860 (1.77s/epoch): Train AUC: 0.7800, Train AP: 0.3760,Test AUC: 0.7387, Test AP: 0.3155\n",
      "Epoch 870 (1.77s/epoch): Train AUC: 0.7810, Train AP: 0.3824,Test AUC: 0.7394, Test AP: 0.3181\n",
      "Epoch 880 (1.77s/epoch): Train AUC: 0.7811, Train AP: 0.3823,Test AUC: 0.7387, Test AP: 0.3161\n",
      "Epoch 890 (1.78s/epoch): Train AUC: 0.7795, Train AP: 0.3712,Test AUC: 0.7362, Test AP: 0.3090\n",
      "Epoch 900 (1.77s/epoch): Train AUC: 0.7779, Train AP: 0.3627,Test AUC: 0.7347, Test AP: 0.3048\n",
      "Epoch 910 (1.77s/epoch): Train AUC: 0.7777, Train AP: 0.3634,Test AUC: 0.7348, Test AP: 0.3053\n",
      "Epoch 920 (1.77s/epoch): Train AUC: 0.7807, Train AP: 0.3778,Test AUC: 0.7375, Test AP: 0.3126\n",
      "Epoch 930 (1.77s/epoch): Train AUC: 0.7805, Train AP: 0.3759,Test AUC: 0.7374, Test AP: 0.3124\n",
      "Epoch 940 (1.77s/epoch): Train AUC: 0.7798, Train AP: 0.3735,Test AUC: 0.7368, Test AP: 0.3110\n",
      "Epoch 950 (1.77s/epoch): Train AUC: 0.7799, Train AP: 0.3730,Test AUC: 0.7366, Test AP: 0.3103\n",
      "Epoch 960 (1.77s/epoch): Train AUC: 0.7790, Train AP: 0.3693,Test AUC: 0.7360, Test AP: 0.3087\n",
      "Epoch 970 (1.77s/epoch): Train AUC: 0.7807, Train AP: 0.3738,Test AUC: 0.7364, Test AP: 0.3095\n",
      "Epoch 980 (1.78s/epoch): Train AUC: 0.7807, Train AP: 0.3739,Test AUC: 0.7359, Test AP: 0.3086\n",
      "Epoch 990 (1.77s/epoch): Train AUC: 0.7796, Train AP: 0.3708,Test AUC: 0.7358, Test AP: 0.3086\n",
      "Epoch 1000 (1.78s/epoch): Train AUC: 0.7813, Train AP: 0.3769,Test AUC: 0.7364, Test AP: 0.3099\n",
      "Running iteration 3 of expt GAE_10d_gat_1000epochs_0.01lr_4_feats\n",
      "{'in_channels': 4, 'out_channels': 10, 'hidden_channels': 10, 'num_layers': 2, 'jk': 'cat'}\n",
      "Epoch 010 (1.79s/epoch): Train AUC: 0.7124, Train AP: 0.3360,Test AUC: 0.6884, Test AP: 0.3202\n",
      "Epoch 020 (1.78s/epoch): Train AUC: 0.6867, Train AP: 0.3139,Test AUC: 0.6509, Test AP: 0.2881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m run_hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m'\u001b[39m: model_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_feats\u001b[39m\u001b[38;5;124m'\u001b[39m: rank_fields,\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m places \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoole\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSouthwark\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_hyperparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m res[\u001b[38;5;28mstr\u001b[39m(include_feats) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-no_deg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/predicting-choice/train.py:296\u001b[0m, in \u001b[0;36mrun_single\u001b[0;34m(places, full_dataset, run_args, save_path)\u001b[0m\n\u001b[1;32m    293\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    294\u001b[0m dataset \u001b[38;5;241m=\u001b[39m train_dataset, test_dataset, train_loader, test_loader\n\u001b[0;32m--> 296\u001b[0m models, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_process_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel training ended. Computing metrics...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    299\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/predicting-choice/train.py:193\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(dataset, data_process_args, model_args, seed, num_iter, save_best_model, save_best_model_metric, output_tb, tag_tb, epochs, print_every, lr, schedule_lr, beta)\u001b[0m\n\u001b[1;32m    191\u001b[0m train_auc, train_ap \u001b[38;5;241m=\u001b[39m test(model, train_dataset)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Inductive metrics\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m test_auc, test_ap \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m    196\u001b[0m result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(total_loss)\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/predicting-choice/train.py:120\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    119\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m--> 120\u001b[0m     auc, ap \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_edge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_edge_label_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     auc_tot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m auc\n\u001b[1;32m    122\u001b[0m     ap_tot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ap\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/torch_geometric/nn/models/autoencoder.py:129\u001b[0m, in \u001b[0;36mGAE.test\u001b[0;34m(self, z, pos_edge_index, neg_edge_index)\u001b[0m\n\u001b[1;32m    125\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pos_pred, neg_pred], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    127\u001b[0m y, pred \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc_score(y, pred), \u001b[43maverage_precision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:232\u001b[0m, in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_label=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid label. It should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpresent_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    229\u001b[0m average_precision \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    230\u001b[0m     _binary_uninterpolated_average_precision, pos_label\u001b[38;5;241m=\u001b[39mpos_label\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:205\u001b[0m, in \u001b[0;36maverage_precision_score.<locals>._binary_uninterpolated_average_precision\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_binary_uninterpolated_average_precision\u001b[39m(\n\u001b[1;32m    203\u001b[0m     y_true, y_score, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m ):\n\u001b[0;32m--> 205\u001b[0m     precision, recall, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Return the step function integral\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# The following works because the last entry of precision is\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# guaranteed to be 1, as returned by precision_recall_curve\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mdiff(recall) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray(precision)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:858\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobas_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     precision \u001b[38;5;241m=\u001b[39m tps \u001b[38;5;241m/\u001b[39m (tps \u001b[38;5;241m+\u001b[39m fps)\n\u001b[1;32m    863\u001b[0m     precision[np\u001b[38;5;241m.\u001b[39misnan(precision)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:754\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    751\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m==\u001b[39m pos_label\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# sort scores and corresponding truth values\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m desc_score_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergesort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    755\u001b[0m y_score \u001b[38;5;241m=\u001b[39m y_score[desc_score_indices]\n\u001b[1;32m    756\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true[desc_score_indices]\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1114\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/wwc4618/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = torch.load(f'{dataset_root}/ssx_dataset.pt')\n",
    "for model_params in [{\n",
    "        'model_type': 'gat',\n",
    "        'out_channels': 10,\n",
    "        'jk': 'cat'\n",
    "    },{\n",
    "        'model_type': 'gat',\n",
    "        'out_channels': 20,\n",
    "    }]:\n",
    "    run_hyperparams = {\n",
    "        'seed': 42,\n",
    "        'model_args': model_params,\n",
    "        'num_iter': 3,\n",
    "        'lr': 0.01,\n",
    "        'epochs': 1000,\n",
    "        'print_every': 10,\n",
    "        'add_deg_feats': False,\n",
    "        'include_feats': rank_fields,\n",
    "    }\n",
    "    places = ['Poole', 'Southwark']\n",
    "    data = run_single(places, dataset, run_args=run_hyperparams)\n",
    "    res[str(include_feats) + '-no_deg'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c99c9-e2d1-4aac-aab5-1f381c270fe0",
   "metadata": {},
   "source": [
    "# Vizualize link prediction on single LAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e41c62-1db0-43c6-9d60-f0abbd6383d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def predict(model, data, enhanced=True):\n",
    "    pos_preds, pos_logits = model.predict_pos(data)\n",
    "    neg_preds, neg_logits = model.predict_neg(data, enhanced=enhanced)\n",
    "    \n",
    "    # Create negative and positive labels\n",
    "    cat_labels = torch.cat([torch.ones(pos_preds.size(0)), torch.zeros(neg_preds.size(0))])\n",
    "    cat_logits = torch.cat([pos_logits, neg_logits] , dim=-1) # TODO\n",
    "    cat_preds = torch.cat([pos_preds, neg_preds] , dim=-1)\n",
    "    cat_index = torch.cat([data.edge_index, data.pos_edge_label_index, data.neg_edge_label_index], dim=-1)\n",
    "    print('Finished prediction, rebuilding road network')\n",
    "    return cat_preds, cat_labels, cat_index\n",
    "    \n",
    "\n",
    "\n",
    "def visualize_preds(place, model, enhanced_predictions=True, hold_out_test_ratio=0.2, neg_sampling_ratio=-1):\n",
    "    original_data = load_graph(place, all_feature_fields)\n",
    "    \n",
    "    data_process_args = {\n",
    "        'split': 1,\n",
    "        'hold_out_edge_ratio': hold_out_test_ratio,\n",
    "        'neg_sampling_ratio': neg_sampling_ratio\n",
    "    }\n",
    "    viz_data = process_dataset([original_data], verbose=False, \n",
    "                               **data_process_args, **model.data_process_args)[0][0]\n",
    "    held_out_edges = [(u.item(), v.item()) for u, v in zip(viz_data.pos_edge_label_index[0], viz_data.pos_edge_label_index[1])]\n",
    "    preds, edge_label, cat_index = predict(model, viz_data, enhanced_predictions)\n",
    "    data = copy.deepcopy(original_data)\n",
    "    \n",
    "    res_dict = {} # For storing new edge attributes in nx\n",
    "    pred_dict = {} # Map of coords to predicted values\n",
    "    label_dict = {} # Map of coords to labels (sanity check)\n",
    "    sampled_dict = {} # For identifying sampled held out edges in plot\n",
    "    \n",
    "    gdf = load_gdf(place)\n",
    "    streets = momepy.gdf_to_nx(gdf, approach='primal', multigraph=False)\n",
    "    pred_streets = momepy.gdf_to_nx(gdf, approach='primal', multigraph=False)\n",
    "    float32_node_dict = {(torch.tensor(c[0], dtype=torch.float32).item(),\n",
    "                          torch.tensor(c[1], dtype=torch.float32).item()): c for c in streets}\n",
    "    count_dict = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}\n",
    "    data = original_data\n",
    "    \n",
    "    # Iterate over predicted edges (which includes real and fake edges)\n",
    "    for i, pred in enumerate(preds):\n",
    "        # Get indices of both the nodes of the edge\n",
    "        u_idx, v_idx = cat_index[:, i]\n",
    "        \n",
    "        # Get their coordinates (the last two node attributes in pretransformed data)\n",
    "        u_float32 = data.x[u_idx, -2].item(), data.x[u_idx, -1].item()\n",
    "        v_float32 = data.x[v_idx, -2].item(), data.x[v_idx, -1].item()\n",
    "        # Convert them into their full precision node coordinates\n",
    "        u, v = float32_node_dict[u_float32], float32_node_dict[v_float32]\n",
    "        key = (u, v)\n",
    "        \n",
    "        \n",
    "        if (u_idx, v_idx) in held_out_edges:\n",
    "            sampled_dict[key] = True\n",
    "        else:\n",
    "            sampled_dict[key] = False\n",
    "        \n",
    "        label = edge_label[i]\n",
    "        if key in pred_dict:\n",
    "            # Should NOT happen\n",
    "            raise NotImplementedError\n",
    "        elif (v, u) in pred_dict:\n",
    "            assert pred_dict[(v, u)] == pred\n",
    "            # Double count for undirected graph, ignore reverse edge\n",
    "            continue\n",
    "        else:\n",
    "            pred_dict[key] = pred\n",
    "        \n",
    "        if not (pred_streets.has_edge(u, v) or pred_streets.has_edge(v, u)):\n",
    "            # Negative sampled edge\n",
    "            assert label == 0\n",
    "            res = 'tn' if pred == label else 'fp'\n",
    "            \n",
    "            # Add the false positive edges for visualization\n",
    "            if res == 'fp':\n",
    "                pred_streets.add_edge(u, v, res=res)\n",
    "        else:\n",
    "            if label != 1:\n",
    "                # Abort mission\n",
    "                raise NotImplementedError(u, v, label)\n",
    "            res = 'tp' if pred == label else 'fn'\n",
    "            res_dict[key] = res\n",
    "\n",
    "        count_dict[res] += 1\n",
    "    print('Results')\n",
    "    print(count_dict)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    tp = count_dict['tp']\n",
    "    fp = count_dict['fp']\n",
    "    fn = count_dict['fn']\n",
    "    prec = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * prec * recall / (prec + recall)\n",
    "    \n",
    "    # Set attributes on the original and predicted graph\n",
    "    nx.set_edge_attributes(pred_streets, res_dict, 'res')\n",
    "    nx.set_edge_attributes(streets, sampled_dict, 'sampled')\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "    for i, facet in enumerate(ax):\n",
    "        facet.set_title((\"Actual\", \"Predicted\")[i], size=20)\n",
    "        facet.axis(\"off\")\n",
    "    # Plot original graph, highlighting the held out edges\n",
    "    colors = ['blue' if edge[2]['sampled'] else 'black' for edge in streets.edges(data=True)]\n",
    "    nx.draw(streets, {n:[n[0], n[1]] for n in list(streets.nodes)}, node_size=0, edge_color=colors, edge_cmap='Set1', ax=ax[0])\n",
    "    \n",
    "    # Plot predicted graph\n",
    "    color_state_map = {'tp': 'green', 'fp': 'blue', 'fn': 'red'}\n",
    "    colors = [color_state_map[edge[2]['res']] for edge in pred_streets.edges(data=True)]\n",
    "    nx.draw(pred_streets, {n:[n[0], n[1]] for n in list(pred_streets.nodes)}, node_size=0, edge_color=colors,\n",
    "            edge_cmap='Set1', ax=ax[1])\n",
    "    plt.suptitle(f'\\\"{place}\\\": {data.num_edges} roads\\n Precision: {prec:.3f}, Recall: {recall:.3f}, F1: {f1: .3f}',\n",
    "                fontweight=\"bold\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "loaded_graphs = {}\n",
    "test_places = ['Poole']\n",
    "model = models[0]\n",
    "torch.save(model.state_dict(), './checkpoint.pt')\n",
    "model = init_model(False, False, 4, 10)\n",
    "model.load_state_dict(torch.load('./checkpoint.pt'))\n",
    "model.data_process_args = {\n",
    "    'add_deg_feats': False,\n",
    "    'include_feats': rank_fields\n",
    "}\n",
    "model = model.eval().to(device)\n",
    "for place in test_places:\n",
    "    visualize_preds(place, model, enhanced_predictions=False, neg_sampling_ratio=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a1b21-81f8-493c-8409-25c05610e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizae link prediction metrics by LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7b971-8b03-4abb-9a31-e1f977e29b8a",
   "metadata": {},
   "source": [
    "# Standard GAE-GCN Link Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539f4c9-d3e1-4908-8730-cafe28d911e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n",
    "                            is_undirected=True, add_negative_train_samples=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(data.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "print_every = 10\n",
    "epochs = 1000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "              f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696449a8-dc93-4064-8676-8f6f7da4cb4b",
   "metadata": {},
   "source": [
    "# ARGVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8bc11-a86d-4f84-9cb7-58c3a6b35b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Linear, ARGVA, GCNConv\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x).relu()\n",
    "        return self.lin3(x)\n",
    "\n",
    "in_channels = dataset[0].num_node_features\n",
    "encoder = Encoder(in_channels, hidden_channels=32, out_channels=32)\n",
    "discriminator = Discriminator(in_channels=32, hidden_channels=64,\n",
    "                              out_channels=32)\n",
    "model = ARGVA(encoder, discriminator).to(device)\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.005)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(),\n",
    "                                           lr=0.001)\n",
    "\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    loss_tot = 0\n",
    "    for data in loader:\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "        # We optimize the discriminator more frequently than the encoder.\n",
    "        for i in range(5):\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss = model.discriminator_loss(z)\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        loss = model.recon_loss(z, data.edge_index)\n",
    "        loss = loss + model.reg_loss(z)\n",
    "        loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        loss_tot += loss\n",
    "    loss_tot /= len(loader)\n",
    "    return float(loss_tot)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(dataset):\n",
    "    model.eval()\n",
    "    auc_tot = 0\n",
    "    ap_tot = 0\n",
    "    for data in dataset:\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        auc, ap = model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "        auc_tot += auc\n",
    "        ap_tot += ap\n",
    "    return auc_tot / len(dataset), ap_tot / len(dataset)\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train(train_loader)\n",
    "    auc, ap = test(test_dataset)\n",
    "    print((f'Epoch: {epoch:03d}, Loss: {loss:.3f}, AUC: {auc:.3f}, '\n",
    "           f'AP: {ap:.3f}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0724a0c-5d20-4d71-a926-9199c341e067",
   "metadata": {},
   "source": [
    "# SEAL + DGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc073f-306e-41b9-9043-d2464f8c21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, split='train'):\n",
    "        self.data = dataset[0]\n",
    "        self.num_hops = num_hops\n",
    "        super().__init__(dataset_root)\n",
    "        index = ['train', 'val', 'test'].index(split)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n",
    "                                    is_undirected=True, split_labels=True)\n",
    "        train_data, val_data, test_data = transform(self.data)\n",
    "\n",
    "        self._max_z = 0\n",
    "\n",
    "        # Collect a list of subgraphs for training, validation and testing:\n",
    "        train_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        # Convert node labeling to one-hot features.\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "        torch.save(self.collate(train_pos_data_list + train_neg_data_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_data_list + val_neg_data_list),\n",
    "                   self.processed_paths[1])\n",
    "        torch.save(self.collate(test_pos_data_list + test_neg_data_list),\n",
    "                   self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)\n",
    "\n",
    "dataset = [data]\n",
    "\n",
    "train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n",
    "val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n",
    "test_dataset = SEALDataset(dataset, num_hops=2, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fd75c-e1fd-47e8-b365-53dca96c4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a11510-b059-4162-abb8-67555b70b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n",
    "\n",
    "\n",
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    val_auc = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1f920-d8e1-4ed1-a69c-c52b53bc3db5",
   "metadata": {},
   "source": [
    "# RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272c22d-90f6-43af-a4bd-9d5ccdacdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn import GAE, RGCNConv\n",
    "\n",
    "class RGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_channels, num_relations):\n",
    "        super().__init__()\n",
    "        self.node_emb = Parameter(torch.Tensor(num_nodes, hidden_channels))\n",
    "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.node_emb)\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = self.node_emb\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DistMultDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_relations, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.rel_emb = Parameter(torch.Tensor(num_relations, hidden_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.rel_emb)\n",
    "\n",
    "    def forward(self, z, edge_index, edge_type):\n",
    "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\n",
    "        rel = self.rel_emb[edge_type]\n",
    "        return torch.sum(z_src * rel * z_dst, dim=1)\n",
    "\n",
    "\n",
    "model = GAE(\n",
    "    RGCNEncoder(data.num_nodes, hidden_channels=500,\n",
    "                num_relations=dataset.num_relations),\n",
    "    DistMultDecoder(dataset.num_relations // 2, hidden_channels=500),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def negative_sampling(edge_index, num_nodes):\n",
    "    # Sample edges by corrupting either the subject or the object of each edge.\n",
    "    mask_1 = torch.rand(edge_index.size(1)) < 0.5\n",
    "    mask_2 = ~mask_1\n",
    "\n",
    "    neg_edge_index = edge_index.clone()\n",
    "    neg_edge_index[0, mask_1] = torch.randint(num_nodes, (mask_1.sum(), ))\n",
    "    neg_edge_index[1, mask_2] = torch.randint(num_nodes, (mask_2.sum(), ))\n",
    "    return neg_edge_index\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    pos_out = model.decode(z, data.train_edge_index, data.train_edge_type)\n",
    "\n",
    "    neg_edge_index = negative_sampling(data.train_edge_index, data.num_nodes)\n",
    "    neg_out = model.decode(z, neg_edge_index, data.train_edge_type)\n",
    "\n",
    "    out = torch.cat([pos_out, neg_out])\n",
    "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "    cross_entropy_loss = F.binary_cross_entropy_with_logits(out, gt)\n",
    "    reg_loss = z.pow(2).mean() + model.decoder.rel_emb.pow(2).mean()\n",
    "    loss = cross_entropy_loss + 1e-2 * reg_loss\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    valid_mrr = compute_mrr(z, data.valid_edge_index, data.valid_edge_type)\n",
    "    test_mrr = compute_mrr(z, data.test_edge_index, data.test_edge_type)\n",
    "\n",
    "    return valid_mrr, test_mrr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_mrr(z, edge_index, edge_type):\n",
    "    ranks = []\n",
    "    for i in tqdm(range(edge_type.numel())):\n",
    "        (src, dst), rel = edge_index[:, i], edge_type[i]\n",
    "\n",
    "        # Try all nodes as tails, but delete true triplets:\n",
    "        tail_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            tail_mask[tails[(heads == src) & (types == rel)]] = False\n",
    "\n",
    "        tail = torch.arange(data.num_nodes)[tail_mask]\n",
    "        tail = torch.cat([torch.tensor([dst]), tail])\n",
    "        head = torch.full_like(tail, fill_value=src)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(tail, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        perm = out.argsort(descending=True)\n",
    "        rank = int((perm == 0).nonzero(as_tuple=False).view(-1)[0])\n",
    "        ranks.append(rank + 1)\n",
    "\n",
    "        # Try all nodes as heads, but delete true triplets:\n",
    "        head_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            head_mask[heads[(tails == dst) & (types == rel)]] = False\n",
    "\n",
    "        head = torch.arange(data.num_nodes)[head_mask]\n",
    "        head = torch.cat([torch.tensor([src]), head])\n",
    "        tail = torch.full_like(head, fill_value=dst)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(head, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        perm = out.argsort(descending=True)\n",
    "        rank = int((perm == 0).nonzero(as_tuple=False).view(-1)[0])\n",
    "        ranks.append(rank + 1)\n",
    "\n",
    "    return (1. / torch.tensor(ranks, dtype=torch.float)).mean()\n",
    "\n",
    "\n",
    "for epoch in range(1, 10001):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}')\n",
    "    if (epoch % 500) == 0:\n",
    "        valid_mrr, test_mrr = test()\n",
    "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
